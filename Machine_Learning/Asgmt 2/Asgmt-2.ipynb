{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Set And Check For NaN \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('Heart_Attack.csv')\n",
    "\n",
    "X = dataset.iloc[:, :-1].values  # Features\n",
    "y = dataset.iloc[:, -1].values   # Heart Atk Risk  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['BMW7812' 67 'Male' ... 'Argentina' 'South America'\n",
      "  'Southern Hemisphere']\n",
      " ['CZE1114' 21 'Male' ... 'Canada' 'North America' 'Northern Hemisphere']\n",
      " ['BNI9906' 21 'Female' ... 'France' 'Europe' 'Northern Hemisphere']\n",
      " ...\n",
      " ['XKA5925' 47 'Male' ... 'Brazil' 'South America' 'Southern Hemisphere']\n",
      " ['EPE6801' 36 'Male' ... 'Brazil' 'South America' 'Southern Hemisphere']\n",
      " ['ZWN9666' 25 'Female' ... 'United Kingdom' 'Europe'\n",
      "  'Northern Hemisphere']]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patient ID                         0\n",
      "Age                                0\n",
      "Sex                                0\n",
      "Cholesterol                        0\n",
      "Blood Pressure                     0\n",
      "Heart Rate                         0\n",
      "Diabetes                           0\n",
      "Family History                     0\n",
      "Smoking                            0\n",
      "Obesity                            0\n",
      "Alcohol Consumption                0\n",
      "Exercise Hours Per Week            0\n",
      "Diet                               0\n",
      "Previous Heart Problems            0\n",
      "Medication Use                     0\n",
      "Stress Level                       0\n",
      "Sedentary Hours Per Day            0\n",
      "Income                             0\n",
      "BMI                                0\n",
      "Triglycerides                      0\n",
      "Physical Activity Days Per Week    0\n",
      "Sleep Hours Per Day                0\n",
      "Country                            0\n",
      "Continent                          0\n",
      "Hemisphere                         0\n",
      "Heart Attack Risk                  0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "missing_values = dataset.isnull().sum() # count of missing values in each column\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Patient ID\n",
      "1: Age\n",
      "2: Sex\n",
      "3: Cholesterol\n",
      "4: Blood Pressure\n",
      "5: Heart Rate\n",
      "6: Diabetes\n",
      "7: Family History\n",
      "8: Smoking\n",
      "9: Obesity\n",
      "10: Alcohol Consumption\n",
      "11: Exercise Hours Per Week\n",
      "12: Diet\n",
      "13: Previous Heart Problems\n",
      "14: Medication Use\n",
      "15: Stress Level\n",
      "16: Sedentary Hours Per Day\n",
      "17: Income\n",
      "18: BMI\n",
      "19: Triglycerides\n",
      "20: Physical Activity Days Per Week\n",
      "21: Sleep Hours Per Day\n",
      "22: Country\n",
      "23: Continent\n",
      "24: Hemisphere\n",
      "25: Heart Attack Risk\n"
     ]
    }
   ],
   "source": [
    "for i, col in enumerate(dataset.columns):\n",
    "    print(f\"{i}: {col}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patient ID                          object\n",
      "Age                                  int64\n",
      "Sex                                 object\n",
      "Cholesterol                          int64\n",
      "Blood Pressure                      object\n",
      "Heart Rate                           int64\n",
      "Diabetes                             int64\n",
      "Family History                       int64\n",
      "Smoking                              int64\n",
      "Obesity                              int64\n",
      "Alcohol Consumption                  int64\n",
      "Exercise Hours Per Week            float64\n",
      "Diet                                object\n",
      "Previous Heart Problems              int64\n",
      "Medication Use                       int64\n",
      "Stress Level                         int64\n",
      "Sedentary Hours Per Day            float64\n",
      "Income                               int64\n",
      "BMI                                float64\n",
      "Triglycerides                        int64\n",
      "Physical Activity Days Per Week      int64\n",
      "Sleep Hours Per Day                  int64\n",
      "Country                             object\n",
      "Continent                           object\n",
      "Hemisphere                          object\n",
      "Heart Attack Risk                    int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(dataset.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data Set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.delete(X_train, [0, 22], axis=1) # Remove Patient ID and Country\n",
    "X_test = np.delete(X_test, [0, 22], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[41 'Male' 374 ... 6 'Asia' 'Northern Hemisphere']\n",
      " [27 'Female' 129 ... 8 'Asia' 'Northern Hemisphere']\n",
      " [65 'Male' 292 ... 4 'Europe' 'Northern Hemisphere']\n",
      " ...\n",
      " [67 'Male' 254 ... 10 'North America' 'Northern Hemisphere']\n",
      " [51 'Male' 399 ... 5 'Asia' 'Northern Hemisphere']\n",
      " [37 'Male' 272 ... 4 'Asia' 'Northern Hemisphere']]\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling & One Hot-Encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Age\n",
      "1: Sex\n",
      "2: Cholesterol\n",
      "3: Blood Pressure\n",
      "4: Heart Rate\n",
      "5: Diabetes\n",
      "6: Family History\n",
      "7: Smoking\n",
      "8: Obesity\n",
      "9: Alcohol Consumption\n",
      "10: Exercise Hours Per Week\n",
      "11: Diet\n",
      "12: Previous Heart Problems\n",
      "13: Medication Use\n",
      "14: Stress Level\n",
      "15: Sedentary Hours Per Day\n",
      "16: Income\n",
      "17: BMI\n",
      "18: Triglycerides\n",
      "19: Physical Activity Days Per Week\n",
      "20: Sleep Hours Per Day\n",
      "21: Continent\n",
      "22: Hemisphere\n"
     ]
    }
   ],
   "source": [
    "for i, col in enumerate(dataset.columns[:-1]):  # Exclude the target column if necessary\n",
    "    print(f\"{i}: {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 36 features, but ColumnTransformer is expecting 24 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[185], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m ct \u001b[38;5;241m=\u001b[39m ColumnTransformer(transformers\u001b[38;5;241m=\u001b[39m[(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoder\u001b[39m\u001b[38;5;124m'\u001b[39m, OneHotEncoder(), categorical_features)], remainder\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      8\u001b[0m X_train \u001b[38;5;241m=\u001b[39m ct\u001b[38;5;241m.\u001b[39mfit_transform(X_train)\n\u001b[1;32m----> 9\u001b[0m X_train \u001b[38;5;241m=\u001b[39m \u001b[43mct\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m X_test \u001b[38;5;241m=\u001b[39m ct\u001b[38;5;241m.\u001b[39mtransform(X_test)\n",
      "File \u001b[1;32mc:\\IIUM\\AI Note IIUM\\venv\\lib\\site-packages\\sklearn\\utils\\_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    325\u001b[0m         )\n",
      "File \u001b[1;32mc:\\IIUM\\AI Note IIUM\\venv\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py:1094\u001b[0m, in \u001b[0;36mColumnTransformer.transform\u001b[1;34m(self, X, **params)\u001b[0m\n\u001b[0;32m   1090\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns are missing: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdiff\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1091\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1092\u001b[0m     \u001b[38;5;66;03m# ndarray was used for fitting or transforming, thus we only\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m     \u001b[38;5;66;03m# check that n_features_in_ is consistent\u001b[39;00m\n\u001b[1;32m-> 1094\u001b[0m     \u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   1096\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _routing_enabled():\n\u001b[0;32m   1097\u001b[0m     routed_params \u001b[38;5;241m=\u001b[39m process_routing(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransform\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n",
      "File \u001b[1;32mc:\\IIUM\\AI Note IIUM\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py:2829\u001b[0m, in \u001b[0;36m_check_n_features\u001b[1;34m(estimator, X, reset)\u001b[0m\n\u001b[0;32m   2826\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   2828\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m estimator\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[1;32m-> 2829\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2830\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2831\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2832\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: X has 36 features, but ColumnTransformer is expecting 24 features as input."
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "categorical_features = [1, 21, 22]  # Column indices for categorical features in X\n",
    "\n",
    "# Apply one-hot encoding\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), categorical_features)], remainder='passthrough')\n",
    "X_train = ct.fit_transform(X_train)\n",
    "X_train = ct.transform(X_train)\n",
    "X_test = ct.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 1.0 'OQD2740' ... 'India' 'Asia' 'Northern Hemisphere']\n",
      " [1.0 0.0 'PMA6847' ... 'Japan' 'Asia' 'Northern Hemisphere']\n",
      " [0.0 1.0 'FBC9961' ... 'Germany' 'Europe' 'Northern Hemisphere']\n",
      " ...\n",
      " [0.0 1.0 'OMJ2303' ... 'United States' 'North America'\n",
      "  'Northern Hemisphere']\n",
      " [0.0 1.0 'YAO7502' ... 'Thailand' 'Asia' 'Northern Hemisphere']\n",
      " [0.0 1.0 'ZMU0934' ... 'Thailand' 'Asia' 'Northern Hemisphere']]\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# # Ensure all features are numeric by selecting only numeric columns\n",
    "# X_train_numeric = np.array(X_train, dtype=float)\n",
    "# X_test_numeric = np.array(X_test, dtype=float)\n",
    "\n",
    "# sc = StandardScaler()\n",
    "# X_train = sc.fit_transform(X_train_numeric)\n",
    "# X_test = sc.transform(X_test_numeric)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
