{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6965e3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import zipfile\n",
    "import tqdm\n",
    "import torchvision.utils as vutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2cbc90d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Already extracted. Skipping extraction.\n"
     ]
    }
   ],
   "source": [
    "zip_path = os.path.join(\"C:/IIUM/AI Note IIUM/Deep_Learning/Week After (W9+)/data\", \"Anime.zip\")\n",
    "extract_path = \"C:/IIUM/AI Note IIUM/Deep_Learning/Week After (W9+)/data/animeface\"\n",
    "\n",
    "# Only extract if not already extracted\n",
    "if not os.path.exists(extract_path) or not os.listdir(extract_path):\n",
    "    print(\"Extracting ZIP file with style...\")\n",
    "\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        files = zip_ref.infolist()\n",
    "\n",
    "        for file in tqdm.tqdm(files, desc=\"Extracting\", unit=\"file\", ncols=80, bar_format=\"{l_bar}{bar} | {n_fmt}/{total_fmt}\"):\n",
    "            zip_ref.extract(file, extract_path)\n",
    "\n",
    "    print(\"✨ Extraction complete.\")\n",
    "else:\n",
    "    print(\"✅ Already extracted. Skipping extraction.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa68d37a",
   "metadata": {},
   "source": [
    "## Main\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7f7d1913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, nz=100, ngf=64, nc=3):\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.ConvTranspose2d(nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()  # Output in range [-1, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n",
    "# Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, nc=3, ndf=64):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()  # Probability output\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input).view(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "adb9fbeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/10][0/249] Loss_D: 1.6305, Loss_G: 1.9992\n",
      "[0/10][50/249] Loss_D: 0.4516, Loss_G: 5.7209\n",
      "[0/10][100/249] Loss_D: 1.5894, Loss_G: 2.3466\n",
      "[0/10][150/249] Loss_D: 0.7059, Loss_G: 3.9036\n",
      "[0/10][200/249] Loss_D: 0.5497, Loss_G: 1.3924\n",
      "[1/10][0/249] Loss_D: 0.5117, Loss_G: 3.3136\n",
      "[1/10][50/249] Loss_D: 0.6407, Loss_G: 5.7470\n",
      "[1/10][100/249] Loss_D: 0.6042, Loss_G: 3.5425\n",
      "[1/10][150/249] Loss_D: 0.3886, Loss_G: 3.8646\n",
      "[1/10][200/249] Loss_D: 0.7147, Loss_G: 3.4017\n",
      "[2/10][0/249] Loss_D: 0.3359, Loss_G: 4.7449\n",
      "[2/10][50/249] Loss_D: 0.6946, Loss_G: 6.2319\n",
      "[2/10][100/249] Loss_D: 1.0030, Loss_G: 2.3795\n",
      "[2/10][150/249] Loss_D: 0.8563, Loss_G: 6.3696\n",
      "[2/10][200/249] Loss_D: 0.5116, Loss_G: 4.3536\n",
      "[3/10][0/249] Loss_D: 1.0649, Loss_G: 2.7839\n",
      "[3/10][50/249] Loss_D: 0.7799, Loss_G: 6.7056\n",
      "[3/10][100/249] Loss_D: 0.9298, Loss_G: 4.3289\n",
      "[3/10][150/249] Loss_D: 1.0074, Loss_G: 8.8861\n",
      "[3/10][200/249] Loss_D: 0.3409, Loss_G: 2.9857\n",
      "[4/10][0/249] Loss_D: 0.2822, Loss_G: 4.7483\n",
      "[4/10][50/249] Loss_D: 0.5077, Loss_G: 5.1959\n",
      "[4/10][100/249] Loss_D: 0.7953, Loss_G: 7.7255\n",
      "[4/10][150/249] Loss_D: 0.6425, Loss_G: 7.2351\n",
      "[4/10][200/249] Loss_D: 0.4007, Loss_G: 3.2088\n",
      "[5/10][0/249] Loss_D: 0.4210, Loss_G: 4.5991\n",
      "[5/10][50/249] Loss_D: 0.7545, Loss_G: 7.7283\n",
      "[5/10][100/249] Loss_D: 2.1453, Loss_G: 11.4820\n",
      "[5/10][150/249] Loss_D: 0.4624, Loss_G: 3.3466\n",
      "[5/10][200/249] Loss_D: 0.2925, Loss_G: 6.5911\n",
      "[6/10][0/249] Loss_D: 0.3588, Loss_G: 5.3200\n",
      "[6/10][50/249] Loss_D: 0.2614, Loss_G: 3.9168\n",
      "[6/10][100/249] Loss_D: 0.3174, Loss_G: 3.9532\n",
      "[6/10][150/249] Loss_D: 0.4683, Loss_G: 2.9647\n",
      "[6/10][200/249] Loss_D: 0.4270, Loss_G: 6.9870\n",
      "[7/10][0/249] Loss_D: 0.2447, Loss_G: 4.0704\n",
      "[7/10][50/249] Loss_D: 0.2030, Loss_G: 4.8573\n",
      "[7/10][100/249] Loss_D: 0.4857, Loss_G: 2.6498\n",
      "[7/10][150/249] Loss_D: 0.3126, Loss_G: 4.1787\n",
      "[7/10][200/249] Loss_D: 0.2594, Loss_G: 4.7453\n",
      "[8/10][0/249] Loss_D: 0.2180, Loss_G: 4.5985\n",
      "[8/10][50/249] Loss_D: 0.1552, Loss_G: 3.9477\n",
      "[8/10][100/249] Loss_D: 0.2473, Loss_G: 6.1545\n",
      "[8/10][150/249] Loss_D: 0.2987, Loss_G: 6.3817\n",
      "[8/10][200/249] Loss_D: 0.1603, Loss_G: 4.7231\n",
      "[9/10][0/249] Loss_D: 0.7754, Loss_G: 10.2168\n",
      "[9/10][50/249] Loss_D: 0.3463, Loss_G: 4.3770\n",
      "[9/10][100/249] Loss_D: 0.2109, Loss_G: 4.0888\n",
      "[9/10][150/249] Loss_D: 0.2952, Loss_G: 5.7347\n",
      "[9/10][200/249] Loss_D: 0.1741, Loss_G: 5.1405\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 256\n",
    "image_size = 64\n",
    "nz = 100\n",
    "num_epochs = 10\n",
    "lr = 0.0002\n",
    "beta1 = 0.5\n",
    "\n",
    "# Dataset loader\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.CenterCrop(image_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Replace this with the correct path to your dataset\n",
    "dataset_path = \"C:/IIUM/AI Note IIUM/Deep_Learning/Week After (W9+)/data/animeface\"\n",
    "dataset = torchvision.datasets.ImageFolder(root=dataset_path, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Models\n",
    "netG = Generator(nz).to(device)\n",
    "netD = Discriminator().to(device)\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizerD = torch.optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizerG = torch.optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "\n",
    "fixed_noise = torch.randn(64, nz, 1, 1, device=device)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (real_images, _) in enumerate(dataloader):\n",
    "        ############################\n",
    "        # (1) Update D network\n",
    "        ###########################\n",
    "        netD.zero_grad()\n",
    "        real_images = real_images.to(device)\n",
    "        b_size = real_images.size(0)\n",
    "        label_real = torch.full((b_size,), 1., device=device)\n",
    "        \n",
    "        output = netD(real_images)\n",
    "        lossD_real = criterion(output, label_real)\n",
    "        lossD_real.backward()\n",
    "\n",
    "        noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
    "        fake_images = netG(noise)\n",
    "        label_fake = torch.full((b_size,), 0., device=device)\n",
    "\n",
    "        output = netD(fake_images.detach())\n",
    "        lossD_fake = criterion(output, label_fake)\n",
    "        lossD_fake.backward()\n",
    "\n",
    "        lossD = lossD_real + lossD_fake\n",
    "        optimizerD.step()\n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network\n",
    "        ###########################\n",
    "        netG.zero_grad()\n",
    "        label_g = torch.full((b_size,), 1., device=device)\n",
    "        output = netD(fake_images)\n",
    "        lossG = criterion(output, label_g)\n",
    "        lossG.backward()\n",
    "        optimizerG.step()\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            print(f\"[{epoch}/{num_epochs}][{i}/{len(dataloader)}] Loss_D: {lossD.item():.4f}, Loss_G: {lossG.item():.4f}\")\n",
    "\n",
    "    # ---- Save only ONCE per epoch ----\n",
    "    output_dir = \"output\"\n",
    "    model_dir = os.path.join(output_dir, \"models\")\n",
    "    sample_dir = os.path.join(output_dir, \"samples\")\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    os.makedirs(sample_dir, exist_ok=True)\n",
    "\n",
    "    vutils.save_image(fake_images.detach(), f\"{sample_dir}/epoch_{epoch:03d}.png\", normalize=True)\n",
    "    torch.save(netG.state_dict(), f\"{model_dir}/netG_epoch_{epoch:03d}.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1ed79827",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_images(model_path, num_images=64, nz=100, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    netG = Generator(nz).to(device)\n",
    "    netG.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    netG.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        noise = torch.randn(num_images, nz, 1, 1, device=device)\n",
    "        fake_images = netG(noise)\n",
    "\n",
    "    # Display a grid of images\n",
    "    grid = vutils.make_grid(fake_images, padding=2, normalize=True)\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Generated Images\")\n",
    "    plt.imshow(grid.permute(1, 2, 0).cpu().numpy())\n",
    "    plt.show()\n",
    "\n",
    "    # Optionally save\n",
    "    vutils.save_image(fake_images, \"generated_output.png\", normalize=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
