{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30f9167e",
   "metadata": {},
   "source": [
    "# Library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bf340638",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b934fc7",
   "metadata": {},
   "source": [
    "# Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ba85ad5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define audio parameters\n",
    "SAMPLE_RATE = 16000  # Assuming 16kHz sample rate\n",
    "N_MFCC = 40  # Number of MFCC features\n",
    "N_FFT = 400  # FFT window size\n",
    "HOP_LENGTH = 160  # Hop length for STFT\n",
    "\n",
    "# Define paths (modify these according to your directory structure)\n",
    "TRAIN_AUDIO_PATH = 'Data_People/Training/'\n",
    "TEST_AUDIO_PATH = 'Data_People/Testing/'\n",
    "\n",
    "# Define classes\n",
    "classes = [\"SUBHANALLAH\", \"ALLAHUAKBAR\", \"ALHAMDULLILAH\"]\n",
    "class_to_idx = {cls: idx for idx, cls in enumerate(classes)}\n",
    "idx_to_class = {idx: cls for idx, cls in enumerate(classes)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409a0831",
   "metadata": {},
   "source": [
    "# Dataset & Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e45f05c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio Dataset Class\n",
    "class AudioWordDataset(Dataset):\n",
    "    def __init__(self, root_dir):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the audio files.\n",
    "        \"\"\"\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.samples = []\n",
    "        \n",
    "        # Check if the root directory exists\n",
    "        if not self.root_dir.exists():\n",
    "            raise FileNotFoundError(f\"Directory {root_dir} does not exist\")\n",
    "        \n",
    "        # Walk through all files in directory\n",
    "        for word_dir in self.root_dir.iterdir():\n",
    "            if word_dir.is_dir() and word_dir.name in classes:\n",
    "                word_class = word_dir.name\n",
    "                for audio_file in word_dir.glob('*.wav'):\n",
    "                    self.samples.append((str(audio_file), class_to_idx[word_class]))\n",
    "        \n",
    "        if len(self.samples) == 0:\n",
    "            print(f\"Warning: No audio files found in {root_dir}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            audio_path, label = self.samples[idx]\n",
    "            \n",
    "            # Load audio with error handling\n",
    "            try:\n",
    "                waveform, sample_rate = torchaudio.load(audio_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading audio file {audio_path}: {e}\")\n",
    "                # Return a dummy waveform and the label\n",
    "                return torch.zeros(1, 16000), label\n",
    "            \n",
    "            # Convert to mono if stereo\n",
    "            if waveform.shape[0] > 1:\n",
    "                waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "            \n",
    "            # Ensure minimum length\n",
    "            if waveform.shape[1] < 1000:  # Arbitrary minimum length\n",
    "                waveform = torch.nn.functional.pad(waveform, (0, 1000 - waveform.shape[1]))\n",
    "            \n",
    "            # Limit maximum length to avoid memory issues\n",
    "            max_length = 16000 * 5  # ~5 seconds at 16kHz\n",
    "            if waveform.shape[1] > max_length:\n",
    "                waveform = waveform[:, :max_length]\n",
    "            \n",
    "            # Extract MFCC features directly here to simplify the pipeline\n",
    "            mfcc_transform = torchaudio.transforms.MFCC(\n",
    "                sample_rate=sample_rate,\n",
    "                n_mfcc=N_MFCC,\n",
    "                melkwargs={\n",
    "                    'n_fft': N_FFT,\n",
    "                    'n_mels': 128,\n",
    "                    'hop_length': HOP_LENGTH,\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            # Extract features and normalize\n",
    "            mfcc = mfcc_transform(waveform)\n",
    "            mean = mfcc.mean()\n",
    "            std = mfcc.std()\n",
    "            mfcc = (mfcc - mean) / (std + 1e-10)\n",
    "            \n",
    "            return mfcc, label\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing sample {idx}: {e}\")\n",
    "            # Return a dummy tensor and the label\n",
    "            return torch.zeros(N_MFCC, 100), 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8824a587",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f9873d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define CNN Model\n",
    "class AudioCNN(nn.Module):\n",
    "    def __init__(self, n_mfcc=N_MFCC, n_classes=len(classes)):\n",
    "        super(AudioCNN, self).__init__()\n",
    "        \n",
    "        # CNN layers\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        \n",
    "        # Adaptive pooling to handle variable size inputs\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((4, 4))\n",
    "        \n",
    "        # FC layers with fixed input size due to adaptive pooling\n",
    "        self.fc1 = nn.Linear(128 * 4 * 4, 256)\n",
    "        self.fc_relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(256, n_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Add channel dimension if necessary\n",
    "        if x.dim() == 3:  # [batch, mfcc, time]\n",
    "            x = x.unsqueeze(1)  # [batch, 1, mfcc, time]\n",
    "        \n",
    "        # CNN\n",
    "        x = self.pool1(self.relu1(self.bn1(self.conv1(x))))\n",
    "        x = self.pool2(self.relu2(self.bn2(self.conv2(x))))\n",
    "        x = self.pool3(self.relu3(self.bn3(self.conv3(x))))\n",
    "        \n",
    "        # Adaptive pooling to handle variable sizes\n",
    "        x = self.adaptive_pool(x)\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # FC\n",
    "        x = self.fc_relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "17699abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified collate function for variable length audio\n",
    "def collate_fn(batch):\n",
    "    # Filter out any None values that might have been returned from __getitem__\n",
    "    batch = [(mfcc, label) for mfcc, label in batch if mfcc is not None]\n",
    "    \n",
    "    if len(batch) == 0:\n",
    "        return torch.zeros(0, 1, N_MFCC, 100).to(device), torch.zeros(0).long().to(device)\n",
    "    \n",
    "    # Separate features and labels\n",
    "    features, labels = zip(*batch)\n",
    "    \n",
    "    # Max length in the time dimension\n",
    "    max_length = max([feature.shape[2] for feature in features])\n",
    "    \n",
    "    # Pad sequences to max length\n",
    "    padded_features = []\n",
    "    for feature in features:\n",
    "        padding_size = max_length - feature.shape[2]\n",
    "        if padding_size > 0:\n",
    "            padded_feature = torch.nn.functional.pad(feature, (0, padding_size))\n",
    "        else:\n",
    "            padded_feature = feature\n",
    "        padded_features.append(padded_feature)\n",
    "    \n",
    "    # Stack tensors\n",
    "    features_tensor = torch.stack(padded_features)\n",
    "    labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "    \n",
    "    return features_tensor, labels_tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d055685",
   "metadata": {},
   "source": [
    "# Data Loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "708e061d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders with safer settings\n",
    "def create_dataloaders(batch_size=16):\n",
    "    try:\n",
    "        # Create datasets with error handling\n",
    "        train_dataset = AudioWordDataset(TRAIN_AUDIO_PATH)\n",
    "        test_dataset = AudioWordDataset(TEST_AUDIO_PATH)\n",
    "        \n",
    "        print(f\"Training samples: {len(train_dataset)}\")\n",
    "        print(f\"Testing samples: {len(test_dataset)}\")\n",
    "        \n",
    "        if len(train_dataset) == 0 or len(test_dataset) == 0:\n",
    "            raise ValueError(\"One or both datasets are empty\")\n",
    "        \n",
    "        # Create data loaders with fewer workers and persistent workers=False for stability\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=batch_size, \n",
    "            shuffle=True,\n",
    "            collate_fn=collate_fn,\n",
    "            num_workers=0,  # Use 0 for debugging, increase if stable\n",
    "            pin_memory=True if torch.cuda.is_available() else False\n",
    "        )\n",
    "        \n",
    "        test_loader = DataLoader(\n",
    "            test_dataset, \n",
    "            batch_size=batch_size, \n",
    "            shuffle=False,\n",
    "            collate_fn=collate_fn,\n",
    "            num_workers=0,  # Use 0 for debugging, increase if stable\n",
    "            pin_memory=True if torch.cuda.is_available() else False\n",
    "        )\n",
    "        \n",
    "        return train_loader, test_loader\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating data loaders: {e}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5590688",
   "metadata": {},
   "source": [
    "# Train & Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4189b406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_one_epoch(model, train_loader, criterion, optimizer, epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    try:\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            try:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                \n",
    "                # Skip empty batches\n",
    "                if inputs.size(0) == 0:\n",
    "                    continue\n",
    "                \n",
    "                # Zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Forward + backward + optimize\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                # Statistics\n",
    "                running_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                \n",
    "                if (i + 1) % 5 == 0:\n",
    "                    print(f'Epoch {epoch+1}, Batch {i+1}: Loss: {running_loss / (i+1):.3f}, Acc: {100 * correct / total:.2f}%')\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error in batch {i}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        epoch_loss = running_loss / max(len(train_loader), 1)\n",
    "        epoch_acc = 100 * correct / max(total, 1)\n",
    "        \n",
    "        return epoch_loss, epoch_acc\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in training epoch {epoch}: {e}\")\n",
    "        return 0.0, 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9553a549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation function\n",
    "def validate(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                try:\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    \n",
    "                    # Skip empty batches\n",
    "                    if inputs.size(0) == 0:\n",
    "                        continue\n",
    "                    \n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    # Statistics\n",
    "                    running_loss += loss.item()\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    total += labels.size(0)\n",
    "                    correct += (predicted == labels).sum().item()\n",
    "                    \n",
    "                    # Save predictions and labels for confusion matrix\n",
    "                    all_preds.extend(predicted.cpu().numpy())\n",
    "                    all_labels.extend(labels.cpu().numpy())\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error in validation batch: {e}\")\n",
    "                    continue\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        val_loss = running_loss / max(len(test_loader), 1)\n",
    "        val_acc = 100 * correct / max(total, 1)\n",
    "        \n",
    "        return val_loss, val_acc, all_preds, all_labels\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in validation: {e}\")\n",
    "        return 0.0, 0.0, [], []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b24a5d",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4779a5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot metrics\n",
    "def plot_metrics(train_losses, val_losses, train_accs, val_accs):\n",
    "    try:\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Plot losses\n",
    "        ax1.plot(train_losses, label='Training Loss')\n",
    "        ax1.plot(val_losses, label='Validation Loss')\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.set_title('Training and Validation Loss')\n",
    "        ax1.legend()\n",
    "        \n",
    "        # Plot accuracies\n",
    "        ax2.plot(train_accs, label='Training Accuracy')\n",
    "        ax2.plot(val_accs, label='Validation Accuracy')\n",
    "        ax2.set_xlabel('Epoch')\n",
    "        ax2.set_ylabel('Accuracy (%)')\n",
    "        ax2.set_title('Training and Validation Accuracy')\n",
    "        ax2.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Error plotting metrics: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9648d020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "def plot_confusion_matrix(y_true, y_pred):\n",
    "    try:\n",
    "        if len(y_true) == 0 or len(y_pred) == 0:\n",
    "            print(\"No data to create confusion matrix\")\n",
    "            return\n",
    "        \n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(\n",
    "            cm, \n",
    "            annot=True, \n",
    "            fmt='d', \n",
    "            cmap='Blues',\n",
    "            xticklabels=classes,\n",
    "            yticklabels=classes\n",
    "        )\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Calculate class-wise accuracy\n",
    "        class_acc = cm.diagonal() / cm.sum(axis=1) * 100\n",
    "        for i, acc in enumerate(class_acc):\n",
    "            print(f'Accuracy for class {idx_to_class[i]}: {acc:.2f}%')\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error plotting confusion matrix: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1dc74e2",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fc4966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 8  # Smaller batch size for stability\n",
    "learning_rate = 0.001\n",
    "num_epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6c2299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders with error handling\n",
    "train_loader, test_loader = create_dataloaders(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8765a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample a batch to verify data shape before proceeding\n",
    "for inputs, labels in train_loader:\n",
    "    print(f\"Sample batch shape: {inputs.shape}, Labels: {labels.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80508ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "# Initialize model\n",
    "model = AudioCNN().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea5a97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        \n",
    "# Training and validation metrics\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accs = []\n",
    "val_accs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f49a423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "    print('-' * 10)\n",
    "        \n",
    "    # Train one epoch\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, epoch)\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "        \n",
    "    # Validate\n",
    "    val_loss, val_acc, all_preds, all_labels = validate(model, test_loader, criterion)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "            \n",
    "    print(f'Training Loss: {train_loss:.4f}, Training Acc: {train_acc:.2f}%')\n",
    "    print(f'Validation Loss: {val_loss:.4f}, Validation Acc: {val_acc:.2f}%')\n",
    "    print()\n",
    "        \n",
    "    # Plot metrics\n",
    "    plot_metrics(train_losses, val_losses, train_accs, val_accs)\n",
    "        \n",
    "    # Plot confusion matrix\n",
    "    print(\"Confusion Matrix:\")\n",
    "    plot_confusion_matrix(all_labels, all_preds)\n",
    "    \n",
    "    print(\"Training and evaluation completed!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
