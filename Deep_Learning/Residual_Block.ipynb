{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04f13c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu121\n",
      "torchvision version: 0.20.1+cu121\n"
     ]
    }
   ],
   "source": [
    "# Import PyTorch\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# Import torchvision\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "# from torchvision.transforms import ToTensor\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Import matplotlib for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check versions\n",
    "# Note: your PyTorch version shouldn't be lower than 1.10.0 and torchvision version shouldn't be lower than 0.11\n",
    "print(f\"PyTorch version: {torch.__version__}\\ntorchvision version: {torchvision.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49e0ed9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "class CIFAR10Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root='./data', train=True, transform=None, download=True):\n",
    "        self.data = torchvision.datasets.CIFAR10(root=root, train=train, download=download, transform=transform)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]  # Returns (image, label) tuple\n",
    "\n",
    "def get_dataloader(batch_size=4, num_workers=0, root='./data'):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "    train_dataset = CIFAR10Dataset(root=root, train=True, transform=transform)\n",
    "    test_dataset = CIFAR10Dataset(root=root, train=False, transform=transform)\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    return train_dataloader, test_dataloader\n",
    "\n",
    "# Get dataloaders\n",
    "train_dataloader, test_dataloader = get_dataloader(batch_size=16, num_workers=0)\n",
    "\n",
    "# Define class names\n",
    "class_names = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dcf96ed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataloaders: (<torch.utils.data.dataloader.DataLoader object at 0x0000020E327CCE20>, <torch.utils.data.dataloader.DataLoader object at 0x0000020E326B8EB0>)\n",
      "Length of train dataloader: 3125 batches of 16\n",
      "Length of test dataloader: 625 batches of 16\n"
     ]
    }
   ],
   "source": [
    "# Let's check out what we've created\n",
    "print(f\"Dataloaders: {train_dataloader, test_dataloader}\")\n",
    "print(f\"Length of train dataloader: {len(train_dataloader)} batches of {batch_size}\")\n",
    "print(f\"Length of test dataloader: {len(test_dataloader)} batches of {batch_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3678df81",
   "metadata": {},
   "source": [
    "## Residual Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8ce88e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "class CIFAR10Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root='./data', train=True, transform=None, download=True):\n",
    "        self.data = torchvision.datasets.CIFAR10(root=root, train=train, download=download, transform=transform)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]  # Returns (image, label) tuple\n",
    "\n",
    "def get_dataloader(batch_size=4, num_workers=0, root='./data'):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "    train_dataset = CIFAR10Dataset(root=root, train=True, transform=transform)\n",
    "    test_dataset = CIFAR10Dataset(root=root, train=False, transform=transform)\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    return train_dataloader, test_dataloader\n",
    "\n",
    "# Get dataloaders\n",
    "train_dataloader, test_dataloader = get_dataloader(batch_size=16, num_workers=0)\n",
    "\n",
    "# Define class names\n",
    "class_names = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adb0413",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def _init_(self, in_channels, out_channels):\n",
    "        super(ResidualBlock, self)._init_() # type: ignore\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # Identity mapping (skip connection)\n",
    "        self.identity = nn.Identity()\n",
    "        if in_channels != out_channels:\n",
    "            self.identity = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.conv1(x))  # Conv1: [B, in_channels, H, W] -> [B, out_channels, H, W]\n",
    "        out = self.conv2(out)  # Conv2: [B, out_channels, H, W] -> [B, out_channels, H, W]\n",
    "        out += self.identity(x)  # Skip connection: [B, out_channels, H, W] + [B, out_channels, H, W]\n",
    "        out = F.relu(out)  # ReLU: [B, out_channels, H, W] -> [B, out_channels, H, W]\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "08ad5b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualCNN(nn.Module):\n",
    "    def _init_(self, num_classes=10):\n",
    "        super(ResidualCNN, self).__init__() # type: ignore\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 6, kernel_size=5)  # Conv1: [B, 3, 28, 28] -> [B, 6, 24, 24]\n",
    "        self.pool = nn.MaxPool2d(2, 2)  # MaxPool: [B, 6, 24, 24] -> [B, 6, 12, 12]\n",
    "        self.res_block1 = ResidualBlock(6, 6)  # ResidualBlock1: [B, 6, 12, 12] -> [B, 6, 12, 12]\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)  # Conv2: [B, 6, 12, 12] -> [B, 16, 8, 8]\n",
    "        self.res_block2 = ResidualBlock(16, 16)  # ResidualBlock2: [B, 16, 8, 8] -> [B, 16, 8, 8]\n",
    "\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # FC1: [B, 16 * 5 * 5] -> [B, 120]\n",
    "        self.fc2 = nn.Linear(120, 84)  # FC2: [B, 120] -> [B, 84]\n",
    "        self.fc3 = nn.Linear(84, num_classes)  # FC3: [B, 84] -> [B, num_classes]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # Conv1 + Pool: [B, 1, 28, 28] -> [B, 6, 12, 12]\n",
    "        x = self.res_block1(x)  # ResidualBlock1: [B, 6, 12, 12] -> [B, 6, 12, 12]\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # Conv2 + Pool: [B, 6, 12, 12] -> [B, 16, 8, 8]\n",
    "        x = self.res_block2(x)  # ResidualBlock2: [B, 16, 8, 8] -> [B, 16, 8, 8]\n",
    "        x = x.view(-1, 16 * 5 * 5)  # Flatten: [B, 16, 8, 8] -> [B, 16 * 5 * 5]\n",
    "        x = F.relu(self.fc1(x))  # FC1: [B, 16 * 5 * 5] -> [B, 120]\n",
    "        x = F.relu(self.fc2(x))  # FC2: [B, 120] -> [B, 84]\n",
    "        x = self.fc3(x)  # FC3: [B, 84] -> [B, num_classes]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6b76ffe4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "optimizer got an empty parameter list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# LOSS AND OPTIMIZER\u001b[39;00m\n\u001b[0;32m      4\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m----> 5\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# move the model to GPU\u001b[39;00m\n\u001b[0;32m      8\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\IIUM\\AI Note IIUM\\venv\\lib\\site-packages\\torch\\optim\\adam.py:78\u001b[0m, in \u001b[0;36mAdam.__init__\u001b[1;34m(self, params, lr, betas, eps, weight_decay, amsgrad, foreach, maximize, capturable, differentiable, fused)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid weight_decay value: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mweight_decay\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     66\u001b[0m defaults \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[0;32m     67\u001b[0m     lr\u001b[38;5;241m=\u001b[39mlr,\n\u001b[0;32m     68\u001b[0m     betas\u001b[38;5;241m=\u001b[39mbetas,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     76\u001b[0m     fused\u001b[38;5;241m=\u001b[39mfused,\n\u001b[0;32m     77\u001b[0m )\n\u001b[1;32m---> 78\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefaults\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fused:\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m differentiable:\n",
      "File \u001b[1;32mc:\\IIUM\\AI Note IIUM\\venv\\lib\\site-packages\\torch\\optim\\optimizer.py:366\u001b[0m, in \u001b[0;36mOptimizer.__init__\u001b[1;34m(self, params, defaults)\u001b[0m\n\u001b[0;32m    364\u001b[0m param_groups \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(params)\n\u001b[0;32m    365\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(param_groups) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 366\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer got an empty parameter list\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(param_groups[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    368\u001b[0m     param_groups \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m: param_groups}]\n",
      "\u001b[1;31mValueError\u001b[0m: optimizer got an empty parameter list"
     ]
    }
   ],
   "source": [
    "model = ResidualCNN()  # Instantiate the model\n",
    "\n",
    "# LOSS AND OPTIMIZER\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# move the model to GPU\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b60f619c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Train in one epoch function\n",
    "def train_one_epoch(model, train_loader, loss_fn, optimizer, device):\n",
    "    model.train()\n",
    "    train_loss, train_correct = 0, 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        train_correct += torch.sum(predictions == labels.data)\n",
    "\n",
    "    return train_loss / len(train_loader.dataset), train_correct.double() / len(train_loader.dataset)\n",
    "\n",
    "# Validation function\n",
    "def validate(model, val_loader, loss_fn, device):\n",
    "    model.eval()\n",
    "    val_loss, val_correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            val_correct += torch.sum(predictions == labels.data)\n",
    "\n",
    "    return val_loss / len(val_loader.dataset), val_correct.double() / len(val_loader.dataset)\n",
    "\n",
    "# Training and validation loop with timing\n",
    "def train_and_validate(model, train_loader, val_loader, loss_fn, optimizer, epochs, device='cuda'):\n",
    "    model.to(device)\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_accuracy': [],\n",
    "        'val_loss': [],\n",
    "        'val_accuracy': []\n",
    "    }\n",
    "\n",
    "    for epoch in tqdm(range(epochs), desc=\"Training Progress\", leave=True):\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        train_loss, train_accuracy = train_one_epoch(model, train_loader, loss_fn, optimizer, device)\n",
    "        val_loss, val_accuracy = validate(model, val_loader, loss_fn, device)\n",
    "\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_accuracy'].append(train_accuracy.item())\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_accuracy'].append(val_accuracy.item())\n",
    "\n",
    "        epoch_end_time = time.time()\n",
    "\n",
    "        # Use tqdm.write() instead of print() to avoid extra blank lines\n",
    "        tqdm.write(f'Epoch {epoch+1}/{epochs}: Train loss: {train_loss:.4f}, Train accuracy: {train_accuracy:.4f}, '\n",
    "                   f'Val loss: {val_loss:.4f}, Val accuracy: {val_accuracy:.4f}, '\n",
    "                   f'Time: {(epoch_end_time - epoch_start_time):.2f}s')\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bdb98343",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'optimizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m\n\u001b[1;32m----> 2\u001b[0m trained_model, history \u001b[38;5;241m=\u001b[39m train_and_validate(model, train_dataloader, test_dataloader, loss_fn, \u001b[43moptimizer\u001b[49m, num_epochs)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'optimizer' is not defined"
     ]
    }
   ],
   "source": [
    "num_epochs =10\n",
    "trained_model, history = train_and_validate(model, train_dataloader, test_dataloader, loss_fn, optimizer, num_epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
